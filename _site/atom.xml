<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Nicholas Rossi</title>
  <link href="http://nicholasarossi.github,io/atom.xml" rel="self"/>
  <link href="http://nicholasarossi.github.io/"/>
  <updated>2020-09-03T17:28:14-07:00</updated>
  <id>http://www.rossidata.com</id>
  <author>
    <name>Nicholas Rossi</name>
    <email>nrossi@bu.edu</email>
  </author>
  
  <entry>
    <title>Uncertainty Quantification Part 1: Ensemble Methods</title>
    <link href="http://nicholasarossi.github.io//UncertaintyQuantificationandEnsembleLearning"/>
    <updated>2020-09-02T00:00:00-07:00</updated>
    <id>http://nicholasarossi.github.io//UncertaintyQuantificationandEnsembleLearning</id>
    <content type="html">&lt;html&gt;

&lt;style&gt;
      h1,h2,h3,head,title {
        font-family: 'open-sans';
        font-weight: 1000;
        outline-color: white;
        color: black;
        background: #ffcccc url(res/blog_17/leaves2.jpg) repeat 0 0;

        text-shadow:
        -.5px -.5px 0 #000,
        .5px -.5px 0 #000,
        -.5px .5px 0 #000,
        .5px .5px 0 #000;
}
&lt;/style&gt;
&lt;/html&gt;

&lt;p&gt;&lt;img src=&quot;../res/blog_18/open_figure_new.png&quot; alt=&quot;/res/blog_18/open_figure_new.png&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;uncertainly-quantification-and-how-much-we-pay-for-lunch&quot;&gt;Uncertainly Quantification and How much we pay for lunch.&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;../res/blog_18/food2-01.png&quot; alt=&quot;/res/blog_18/food.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Uncertainly quantification can be something of an abstract concept, but let’s try to anchor it to a simple question.&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;Can&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;you&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;guess&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;how&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;much&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;someone&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;spent&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;on&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lunch&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;based&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;on&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;how&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;much&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;they&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;make&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;?&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;While the initial intuition might be “yeah it simply correlates with how much they earn” - consider this : rich people still eat the occasional fast food. Mark ZUCC might have a banana and some bread for lunch (cheap) or he might eat some endangered animal (expensive). However, poor people don’t have those kind of options.  They’re basically restricted to budgeting on every meal. In other words, the uncertainty of how much someone spends on a meal increases as a function of their income. This is an example of &lt;a href=&quot;https://en.wikipedia.org/wiki/Heteroscedasticity&quot;&gt;heteroscedastic&lt;/a&gt; uncertainty : variability that is not uniform across the feature space. If we were tasked with building a lunch price forecasting model, it may be necessary to quantify this kind of uncertainty.&lt;/p&gt;

&lt;p&gt;Consider the following graphs:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../res/blog_18/meal_expenses.png&quot; alt=&quot;/res/blog_18/meal_expenses.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The graph to the left shows some raw imagined data relating how wealthy someone is to how much they spent on their last meal (grey dots). While there is a clear trend line (turquoise), this fails to capture the uniqueness of the data : that the uncertainly from that trend increases as a function of wealth as well. The graph to the right shows the perfect analytical mapping of this data. Using the closed form solution to the standard deviation of a uniform distribution (how the data was generated) we see the goal of our uncertainty model - to perfectly capture the uncertainty.&lt;/p&gt;

&lt;p&gt;The important thing is that &lt;em&gt;uncertainty&lt;/em&gt; is part of this system. Its not something we want to filter out, its’ something we want to quantify. The only question is how do we do it? For our first attempt, we will use a collection of linear models, collated into an ensemble.&lt;/p&gt;

&lt;h3 id=&quot;using-ensemble-learning-to-quantify-uncertainty--linear-models&quot;&gt;Using ensemble learning to quantify uncertainty : linear models&lt;/h3&gt;

&lt;p&gt;Ensemble models are simply meta machine learning models built from several smaller models. These individual member models can all have the same or different architectures and be trained on smaller slices of the total training dataset. Each member model then offers up a prediction as to what it thinks the solution will be. All the member predictions are then synthesized to form a final prediction based on taking some average or biased sum of all the members. The uncertainty measure is derived from how much these member models disagree with each other.&lt;/p&gt;

&lt;p&gt;The simplest implementation of this that I’ll start with is an ensemble model built from a series of linear regressions, each trained on a different subset of the training data.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../res/blog_18/LINEAR_uq-01.png&quot; alt=&quot;/res/blog_18/LINEAR_uq-01.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;A very simple implementation of this can be seen below. Here we train 5 (n_splits=5) linear regression models with shuffle split fractions of the data. In the predict method we then use each of these models to find a solution and then take the mean response. The &lt;em&gt;uncertainty&lt;/em&gt; is simply the standard deviation of predictions around this mean.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;LinearUncertaintyPredictor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_train&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_data&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_train&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_data&lt;/span&gt;
        
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;train_models&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_splits&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;kernal&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'linear'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;s&quot;&gt;'''Function for training models 
        Inputs:
        X_dataset: Inputs of the training data (MxN)
        y_dataset: Outputs of the trainig data (1xN)
        params: the parameters of the LGBM model
        n_split: the number of splits to be used to creat int(n_splits) seperate models

        Returns:
        models: int(n_splits) number of independent models trained on the folds of the data
        '''&lt;/span&gt;

        &lt;span class=&quot;n&quot;&gt;ss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ShuffleSplit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;test_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_splits&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_splits&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_splits&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random_state&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;88&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

        &lt;span class=&quot;n&quot;&gt;models&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;coeffs&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;train_index&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;test_index&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;split&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;X_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X_val&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train_index&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,:],&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;test_index&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,:],&lt;/span&gt; 
            &lt;span class=&quot;n&quot;&gt;y_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_val&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train_index&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;test_index&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;reg&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;LinearRegression&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit_intercept&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
            
            &lt;span class=&quot;n&quot;&gt;models&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;coeffs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;coef_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;models&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;models&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;coefficients&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;coeffs&lt;/span&gt;
        
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;predict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;predictions&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;models&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;predictions&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;predict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
        
        &lt;span class=&quot;c1&quot;&gt;# Values chosen by mean committe
&lt;/span&gt;        &lt;span class=&quot;n&quot;&gt;ensemble_predictions&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;stack&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;predictions&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;ensemble_uncertainty&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;stack&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;predictions&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;predictions&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'prediction'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ensemble_predictions&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'uncertainty'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ensemble_uncertainty&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;raw_predictions&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'X'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;squeeze&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;stack&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;predictions&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))}&lt;/span&gt;
        
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;predictions&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;To evaluate this model, we simply split off a true holdout set and then train our ensemble on the remaining fraction.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;X_dataset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X_holdout&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_dataset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_holdout&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;train_test_split&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_values&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_values&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;test_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random_state&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;88&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;Predictor&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Linear_Uncertainty_Predictor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_dataset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_dataset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;Predictor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train_models&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_splits&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Now, let’s see how it performed.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../res/blog_18/linear_model_summary.png&quot; alt=&quot;/res/blog_18/linear_model_summary.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;First, the “prediction” of this model seems to correlate perfectly with our expected trend (left). But that’s no surprise. What’s more important is that our uncertainty correlates monotonically with income (right). This shows that our ensemble effectively did map out the underlying uncertainty of the data as a function of the feature space. This shows that even a series of simple member models can reveal interesting things about the uncertainty of a dataset.&lt;/p&gt;

&lt;h3 id=&quot;more-sophisiticated-models--lightgbm&quot;&gt;More sophisiticated models : LightGBM&lt;/h3&gt;

&lt;p&gt;In order to show how more flexible base models impact the results, we simply swap out the linear regression for LightGBM (The code and notebooks can be found &lt;a href=&quot;https://github.com/NicholasARossi/UQ_methods&quot;&gt;here&lt;/a&gt;). Tree based algorithms like light GBM offer a lot more flexibility for complex, non-linear relationships. Below we see how a 5 member LGBM ensemble model looks after being trained.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../res/blog_18/multiplanel.png&quot; alt=&quot;/res/blog_18/multiplanel.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We see that each model becomes more erratic in its predictions as the income of the person increases, with the ensemble model showing higher error bars indicating higher uncertainty.&lt;/p&gt;

&lt;p&gt;Furthermore, the animation below shows how the individual preditions move a as a function of the member model, with predictions for higher earners moving them most.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../res/blog_18/LGBM.gif&quot; alt=&quot;/res/blog_18/LGBM.gif&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Together, we see incorporating more complex or flexible models enables accurate recapitulation of underlying data uncertainty.&lt;/p&gt;

&lt;h3 id=&quot;real-data-multiple-features-and-stratification&quot;&gt;Real Data, Multiple Features and Stratification&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;../res/blog_18/boston-01.png&quot; alt=&quot;/res/blog_18/boston-01.png&quot; /&gt;
So far we’ve only looked at a synthetic data set with one input feature and one output, but these sort of uncertainty quantification methods generalize well to more complex datasets. To illustrate this, let’s take a look at the boston housing dataset, one of the canonical datasets for regression problems.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sklearn.datasets&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;load_boston&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;load_boston&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;return_X_y&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Not only does this dataset not have artificial levels of uncertainty, it also has multiple input features that can be used to predict the price of a house in Boston (location, crime statistics, schooling etc). One additional point of interest is the “skew” of the dataset, that is to say that there aren’t equal numbers of every price of home. Broadly, this is described as the &lt;em&gt;imbalanced class&lt;/em&gt; problem, and it works for continuous data as well. This can pose a real problem for this sort of uncertainty quantification in certain circumstances so it’s an important thing to keep in mind. The figure below illustrates the skew of the data.
&lt;img src=&quot;../res/blog_18/sampling_figure.png&quot; alt=&quot;/res/blog_18/sampling_figure.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The turquoise line illustrates the shape of all training data, with most houses priced towards the lower end of the arbitrary scale, with a fat tail of higher priced homes. We need to sample from this distribution for the data subsets for our candidate models - but which technique to use? Broadly we could use &lt;strong&gt;k-fold&lt;/strong&gt; or &lt;strong&gt;shuffle&lt;/strong&gt; splitting to pull random samples from the underlying distribution (pink line). Or we could attempt to stratify sample this data (grey line). Stratify sampling attempts to sample evenly throughout the space. It involves first binning the data and then assuring that you get an even number of samples from each bin. A minimal implementation of a stratified split helper function can be seen below.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;continuous_stratification&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_bins&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;test_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;'''Function that returns the stratified indexes of a continously valued y'''&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;bins&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linspace&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;min&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;max&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_bins&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;digitized_y&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;digitize&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bins&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;index_list&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;arange&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;stratified_indexes&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;n_samples&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;test_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;bucket_list&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;set&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;digitized_y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;n_samples_per_bucket&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_samples&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bucket_list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bucket&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;set&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;digitized_y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;sub_index_list&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;index_list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;digitized_y&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;==&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bucket&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;sample_indexes&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;choice&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sub_index_list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_samples_per_bucket&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;replace&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;stratified_indexes&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sample_indexes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# returns test and train indexes
&lt;/span&gt;    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;stratified_indexes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setdiff1d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;index_list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;stratified_indexes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;So the question is now : how do these sampling techniques add up? To test them, we trained a 5 member LGBM ensemble using shuffle splitting, stratified splitting or kfold to resample the data subsets to train the models with. Then we measured their performance against a holdout set.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../res/blog_18/multi_sample_predict.png&quot; alt=&quot;/res/blog_18/multi_sample_predict.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;So it actually seems like the stratified sample has the worst correlation between predicted and true values (should be 1:1). But remember that’s only half of the functionality of a model like this. We should be seeing correlations between uncertainty and error - suggesting this model is self aware of the mistakes it’s making.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../res/blog_18/multi_sample_error.png&quot; alt=&quot;/res/blog_18/multi_sample_error.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This figure shows little correlation between any of these models uncertainty and error, however k-fold performs the best. This highlights that while these methods have clear merit on synthetic data, they struggle more with real world examples where the uncertainty is more subtle.&lt;/p&gt;

&lt;h3 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h3&gt;

&lt;p&gt;This served as an introduction to the power of ensemble methods for uncertainty quantification. While they have their limitations, they serve a low complexity solution to this burgeoning field of machine learning. Future posts will focus on more complex implimentations of PyMC3 and Tensorflow probability to solve similar problems.&lt;/p&gt;

&lt;h3 id=&quot;notes&quot;&gt;Notes&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Notebook necessary to generate all the graphs and more &lt;a href=&quot;https://github.com/NicholasARossi/UQ_methods/blob/master/notebooks/Ensemble_methods.ipynb&quot;&gt;here&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;The rest of the model and helper scripts &lt;a href=&quot;https://github.com/NicholasARossi/UQ_methods&quot;&gt;here&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

</content>
    <author>
      <name>Nicholas Rossi</name>
      <uri>http://nicholasarossi.github,io/about/</uri>
    </author>
  </entry>
  
  <entry>
    <title>How the Financial Crisis Hurt the Poor and Helped the Rich</title>
    <link href="http://nicholasarossi.github.io//FinancialCrisis"/>
    <updated>2018-08-19T00:00:00-07:00</updated>
    <id>http://nicholasarossi.github.io//FinancialCrisis</id>
    <content type="html">&lt;html&gt;
&lt;link rel=&quot;stylesheet&quot; href=&quot;https://fonts.googleapis.com/css?family=Indie+Flower&quot; /&gt;
&lt;style&gt;

      h1,h2,h3,head,title {

        font-family: 'Indie Flower',serif;
        outline-color: white;
        color: black;
        background: black url(res/blog_17/leaves2.jpg) repeat 0 0;

        &lt;!-- background-color: slategrey; --&gt;
}
&lt;/style&gt;
&lt;/html&gt;

&lt;h3 id=&quot;winners-and-losers&quot;&gt;Winners and Losers&lt;/h3&gt;

&lt;p&gt;The 2007 financial crisis affected each of us differently, with the “winners” and “losers” falling sharply
into economic lines. The graph below compares how the bottom 20% of income earners with the top 10%. Adjusted for inflation,
there is an obvious difference in how the two group faired through the financial crisis (highlighted in gray).
&lt;a href=&quot;res/blog_17/wealth.png&quot;&gt;
&lt;img src=&quot;res/blog_17/wealth.png&quot; /&gt;
&lt;/a&gt;﻿&lt;/p&gt;

&lt;p&gt;Why did this happen? Why did the poor do so much worse through the crisis? The answer lies in how people keep their assets.&lt;/p&gt;

&lt;h3 id=&quot;home-is-where-the-heart-is-if-youre-poor&quot;&gt;Home is where the heart is… if you’re poor&lt;/h3&gt;

&lt;p&gt;Looking at just the most recent data from 2016, we see that the majority of lower income earners have almost all of their wealth in their home.
Very little of their total wealth is in financial products such as stocks, bonds, futures etc.&lt;/p&gt;
&lt;iframe src=&quot;res/blog_17/rich-and-poor-pie2/index.html&quot; height=&quot;500&quot; width=&quot;100%&quot; scrolling=&quot;no&quot;&gt;&lt;/iframe&gt;

&lt;p&gt;Compare this to the top earners which have a sizable portion of their total assets tied up in financial products:&lt;/p&gt;
&lt;iframe src=&quot;res/blog_17/rich-and-poor-pie/index.html&quot; height=&quot;500&quot; width=&quot;100%&quot; scrolling=&quot;no&quot;&gt;&lt;/iframe&gt;

&lt;p&gt;This difference is not merely cosmetic, it had a profound impact on why the rich did better in the recession.&lt;/p&gt;

&lt;h3 id=&quot;no-one-was-totally-spared-from-the-mortgage-crisis&quot;&gt;No one was totally spared from the mortgage crisis&lt;/h3&gt;

&lt;p&gt;Despite the total wealth increasing through the recession, the rich did lose money on their homes as the poor did when the bubble burst in 2007.
The graph below show the median wealth tied up in the homes of both groups.
&lt;a href=&quot;res/blog_17/housing.png&quot;&gt;
&lt;img src=&quot;res/blog_17/housing.png&quot; /&gt;
&lt;/a&gt;﻿&lt;/p&gt;

&lt;h3 id=&quot;financial-instruments-are-the-refuge-of-the-wealthy&quot;&gt;Financial instruments are the refuge of the wealthy&lt;/h3&gt;

&lt;p&gt;However, the rich were mostly insulated from the affects of the crisis thanks to their financial investment which accrued value through the recession.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;res/blog_17/finance.png&quot;&gt;
&lt;img src=&quot;res/blog_17/finance.png&quot; /&gt;
&lt;/a&gt;﻿&lt;/p&gt;

&lt;p&gt;This illustrates how the difference in how both groups responded to the financial catastrophe can be primarily attributed to a difference in how their assets were allocated.&lt;/p&gt;

&lt;h3 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h3&gt;

&lt;p&gt;The net result of the assets of the rich being protected was expected but not inevitable. The post-2007 recovery policies that focused on distribution of the 700+ billion dollars allocated
 by congress favored the preservation of financial institutions over the welfare of individual citizens. While the Obama administration was relatively “left” leaning compared to what McCain’s
 platform was, it seems the net product of his stimulus allocation was indistinguishable from what a hypothetical republican administration would have done. For more information, watch the
 video below outlining these results:&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=JE3KrmFy1u4&quot;&gt;
&lt;img src=&quot;res/blog_17/obama_thumb.png&quot; /&gt;
&lt;/a&gt;﻿&lt;/p&gt;

&lt;h3 id=&quot;notes&quot;&gt;Notes&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Code used to generate these graphs can be found &lt;a href=&quot;https://github.com/NicholasARossi/VizSnacks&quot;&gt;here&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Data taken from the &lt;a href=&quot;https://www.federalreserve.gov/econres/scfindex.htm&quot;&gt;survey of consumer finances&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

</content>
    <author>
      <name>Nicholas Rossi</name>
      <uri>http://nicholasarossi.github,io/about/</uri>
    </author>
  </entry>
  
  <entry>
    <title>Roman Emperor Scrollytelling</title>
    <link href="http://nicholasarossi.github.io//RomanEmperors"/>
    <updated>2018-06-20T00:00:00-07:00</updated>
    <id>http://nicholasarossi.github.io//RomanEmperors</id>
    <content type="html">&lt;p&gt;For an r/dataisbeautiful challenge I took a look at data surrounding the roman emperors.
Click on the image below to check it out:&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;res/blog_11_romans/emperor_scroll/scrollytelling.html&quot;&gt;
&lt;img src=&quot;res/blog_11_romans/vapor.JPG&quot; /&gt;
&lt;/a&gt;﻿&lt;/p&gt;
</content>
    <author>
      <name>Nicholas Rossi</name>
      <uri>http://nicholasarossi.github,io/about/</uri>
    </author>
  </entry>
  
  <entry>
    <title>Easing Animations with Python</title>
    <link href="http://nicholasarossi.github.io//PythonAnimations"/>
    <updated>2018-06-10T00:00:00-07:00</updated>
    <id>http://nicholasarossi.github.io//PythonAnimations</id>
    <content type="html">&lt;link rel=&quot;stylesheet&quot; href=&quot;/res/blog_15/manni.css&quot; /&gt;

&lt;style&gt;
      h1,h2,h3,head,title {
        font-weight: 1000;
        outline-color: black;
        color: black;
        background-color: #e684ae;
}
		iframe {
			width: 10px;
			min-width: 100%;
			*width: 100%;
		}
&lt;/style&gt;

&lt;p&gt;&lt;a href=&quot;/res/blog_15/africa_animated (2).svg&quot;&gt;
&lt;img src=&quot;/res/blog_15/africa_animated (2).svg&quot; /&gt;
&lt;/a&gt;﻿&lt;/p&gt;

&lt;h3 id=&quot;building-animations-in-python&quot;&gt;Building Animations in Python&lt;/h3&gt;

&lt;p&gt;While they may be over-hyped these days, animations are a key form of data visualization. Some things just look better changing over time. Also, while some languages like d3.js or processing put a focus on animations first, Python remains relatively clunky to use if you want to make smooth animations that pop.&lt;/p&gt;

&lt;p&gt;Take for instance an example animation like the one created exclusively in python below:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python3&quot; data-lang=&quot;python3&quot;&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;import matplotlib.pyplot as plt
import numpy as np
from matplotlib import animation
plt.close('all')
colors = [ 'teal']

fig_animate, ax = plt.subplots()
dots = []

dots.append(ax.plot([], [], linestyle='none', marker='h', markersize=30, color=colors[0]))

ax.set_xlim([-1,11])
ax.set_ylim([-1,11])

data=np.round(3*np.sin(np.linspace(0,6*np.pi,100))+5)

def animate(z):
    dots[0][0].set_data(data[z],data[z])
    return dots

anim = animation.FuncAnimation(fig_animate, animate, frames=len(data), blit=False)

ax.set_facecolor('#d3d3d3')
writer = animation.writers['ffmpeg'](fps=10)
dpi=300

anim.save('dot.mp4', writer=writer,dpi=dpi)
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;&lt;img src=&quot;/res/blog_15/gif.gif&quot; alt=&quot;dots&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;the-power-of-easing&quot;&gt;The power of Easing&lt;/h3&gt;

&lt;p&gt;We see that indeed things are moving over time, but they don’t have that orgainic feel that we’ve come to expect from visual data.&lt;/p&gt;

&lt;p&gt;The big thing absent here is &lt;strong&gt;easing&lt;/strong&gt;. Easing is the process of interpolating between two points in order for the animation to no longer look jerky. Use the interactive below to explore a few differnt types of easing.&lt;/p&gt;

&lt;iframe src=&quot;/res/blog_15/easing_javascript.html&quot; width=&quot;100%&quot; height=&quot;300px&quot; scrolling=&quot;no&quot;&gt;&lt;/iframe&gt;

&lt;p&gt;We see intuitively than any form of interpolation between point A and point B make the whole thing feel a lot more natural - it takes away the appearance of a slide show.&lt;/p&gt;

&lt;h3 id=&quot;implementing-easing-in-python&quot;&gt;Implementing Easing in Python&lt;/h3&gt;

&lt;p&gt;I wrote a small packages that facilitates easing in python by adding differnt types of smoothing between datapoints.&lt;/p&gt;

&lt;p&gt;This is how it works:&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/res/blog_15/interpolation_schema.png&quot;&gt;
&lt;img src=&quot;/res/blog_15/interpolation_schema.png&quot; /&gt;
&lt;/a&gt;﻿&lt;/p&gt;

&lt;p&gt;All you have to do to make it work is pass in an original datavector, it’s time vector and and output vector. Then execute one of the interpolation functions on it and it will return nice interpolated data:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;ease&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Eased&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;input_time_vector&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;output_time_vector&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;out_data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ease&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;power_ease&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;All the code necessary to implement this can be found &lt;a href=&quot;https://github.com/NicholasARossi/Easing-Animations-with-Python&quot;&gt;here&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&quot;applying-our-tools-to-african-conflict-data&quot;&gt;Applying our tools to African Conflict Data&lt;/h3&gt;

&lt;p&gt;So what does this stuff look like on real data? Below are some graphs that show violent conflict within the african conflict over time with various types of easing.&lt;/p&gt;

&lt;p&gt;The first is just a barchart of the cummulative dead over time:&lt;/p&gt;

&lt;video controls=&quot;&quot; loop=&quot;&quot; autoplay=&quot;&quot; width=&quot;100%&quot;&gt;
&lt;source src=&quot;/res/blog_15/total_dead.mp4&quot; autoplay=&quot;true&quot; type=&quot;video/mp4&quot; /&gt;

Your browser does not support the video tag.
&lt;/video&gt;

&lt;p&gt;The second is a distribution of the fatalities per event over time:&lt;/p&gt;

&lt;video controls=&quot;&quot; loop=&quot;&quot; autoplay=&quot;&quot; width=&quot;100%&quot;&gt;
&lt;source src=&quot;/res/blog_15/hist_only.mp4&quot; autoplay=&quot;true&quot; type=&quot;video/mp4&quot; /&gt;

Your browser does not support the video tag.
&lt;/video&gt;

&lt;p&gt;This final animation brings several types together and shows how you can impliment but 1d and 2d easing.&lt;/p&gt;

&lt;video controls=&quot;&quot; loop=&quot;&quot; autoplay=&quot;&quot; width=&quot;100%&quot;&gt;
&lt;source src=&quot;/res/blog_15/conflict_upload.webm&quot; autoplay=&quot;true&quot; type=&quot;video/webm&quot; /&gt;
&lt;!-- &lt;source src=&quot;/res/blog_15/conflict_upload.mp4&quot; autoplay=&quot;true&quot; type=&quot;video/mp4&quot;&gt; --&gt;

Your browser does not support the video tag.
&lt;/video&gt;

&lt;h3 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h3&gt;

&lt;p&gt;Watch the video below for a presentation on this material:&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=6GxWgJATj78&quot;&gt;
&lt;img src=&quot;/res/blog_15/python_animate_thumb.png&quot; /&gt;
&lt;/a&gt;﻿&lt;/p&gt;

&lt;h3 id=&quot;notes&quot;&gt;Notes:&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/NicholasARossi/Easing-Animations-with-Python&quot;&gt;Github Repo with animation library&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;African conflict data from &lt;a href=&quot;https://www.kaggle.com/jboysen/african-conflicts/kernels&quot;&gt;here&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</content>
    <author>
      <name>Nicholas Rossi</name>
      <uri>http://nicholasarossi.github,io/about/</uri>
    </author>
  </entry>
  
  <entry>
    <title>Flight Delays</title>
    <link href="http://nicholasarossi.github.io//flightdelays"/>
    <updated>2018-06-01T00:00:00-07:00</updated>
    <id>http://nicholasarossi.github.io//flightdelays</id>
    <content type="html">&lt;html&gt;
&lt;link rel=&quot;stylesheet&quot; href=&quot;https://fonts.googleapis.com/css?family=Playfair+Display&quot; /&gt;
&lt;style&gt;

      h1,h2,h3,head,title {
        font-family: 'playfair display';
        outline-color: white;
        color: white;
        background-color: slategrey;
}
&lt;/style&gt;
&lt;/html&gt;

&lt;h3 id=&quot;the-worst-airports-in-america&quot;&gt;The Worst Airports in America&lt;/h3&gt;

&lt;p&gt;What makes a bad airport? There’s a lot that can go wrong with a flight - like having to ride in a snow crawler as in &lt;a href=&quot;https://en.wikipedia.org/wiki/Mobile_lounge&quot;&gt;Washington Dulles&lt;/a&gt;. But we can all agree that having your flight delayed is one of the worst things.&lt;/p&gt;

&lt;p&gt;By extension, looking at the number of delays per airport is a good way to rank the quality of those airports. The chart below shows the worst airports for total numbers of delays.&lt;/p&gt;

&lt;iframe src=&quot;/res/blog_16/bar_wrapper.html&quot; height=&quot;400px&quot; width=&quot;100%&quot; scrolling=&quot;no&quot;&gt;&lt;/iframe&gt;

&lt;p&gt;Anyone that’s been to O’hare shouldn’t be suprised by this. I should note that I scraped this data over two weeks in April 2018. But there’s no reason to believe this data isn’t representative of the norm as there were no big weather events.&lt;/p&gt;

&lt;p&gt;Clearly, we’ve selected a lot of big airports here. So naturally the next question is which airports have the worst percentage of delays.&lt;/p&gt;

&lt;p&gt;Note, I just looked at airports that had more than 1000 flights over the last two weeks. That removed all the small air-feilds with bad track records. I’m not interested in airports that made 100% or 0% of their on-time numbers if they only had a few flights period.&lt;/p&gt;

&lt;iframe src=&quot;/res/blog_16/bar_wrapper2.html&quot; height=&quot;400px&quot; width=&quot;100%&quot; scrolling=&quot;no&quot;&gt;&lt;/iframe&gt;

&lt;p&gt;Surprisingly, O’hare isn’t even on this list. Take that Midway! Memphis takes the cake with an astounding 35% of flights delayed.&lt;/p&gt;

&lt;h3 id=&quot;case-study-new-york-airports&quot;&gt;Case Study: New York Airports&lt;/h3&gt;

&lt;p&gt;Let’s just consider for a second some comparable airports - those that surround NYC. They each serve comparable volume with JFK and EWR each running ~4000 fights a week and LGA launching ~3500 (data averaged over 2 weeks in april). Now if you’re going to book a ticket through one of them which should you pick?
Well, JFK is the best in terms of on-time flights with just slightly below the national average for on-time rates. However, they’re not radically different from each other compared to the outlier major airports of Memphis and Minneapolis.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/res/blog_16/insta-01.png&quot;&gt;
&lt;img src=&quot;/res/blog_16/insta-01.png&quot; /&gt;
&lt;/a&gt;﻿&lt;/p&gt;

&lt;h3 id=&quot;relationship-between-size-of-airport-and-number-of-delays&quot;&gt;Relationship between size of airport and number of delays?&lt;/h3&gt;

&lt;p&gt;Does a bigger airport necessarily mean more delays? Not so much. You can explore the data below to find out for yourself. Mouse over to see airport details. The toggle switch changes between linear and log axis for the number of airport delays (this helps visualize the data). The size of the dots scales with the number of total flights from that airport.&lt;/p&gt;

&lt;!-- &lt;iframe src=&quot;/res/blog_16/scatter.html&quot; height=&quot;400px&quot; width=&quot;100%&quot; scrolling=&quot;no&quot; &gt;&lt;/iframe&gt; --&gt;

&lt;style&gt;.embed-container { position: relative; padding-bottom: 400px; height: 0; overflow: hidden; max-width: 100%; } .embed-container iframe, .embed-container object, .embed-container embed { position: absolute; top: 0; left: 0; width: 100%; height: 100%; }&lt;/style&gt;
&lt;div class=&quot;embed-container&quot;&gt;&lt;iframe src=&quot;/res/blog_16/scatter.html&quot; frameborder=&quot;0&quot; allowfullscreen=&quot;&quot; scrolling=&quot;no&quot;&gt;&lt;/iframe&gt;&lt;/div&gt;

&lt;p&gt;Turns out Portland and Minneapolis are both really good for their size, while Memphis, Midway and Philly aren’t so good.&lt;/p&gt;

&lt;h3 id=&quot;locations-of-bad-airports&quot;&gt;Locations of bad airports&lt;/h3&gt;

&lt;p&gt;To visualize this spatially, we can plot the total number of delays by location. The animation cycles between total delays, the percentage delayed at those airports and the state in which they’re located.&lt;/p&gt;

&lt;iframe src=&quot;/res/blog_16/flight_wrapper.html&quot; height=&quot;400px&quot; width=&quot;100%&quot; scrolling=&quot;no&quot;&gt;&lt;/iframe&gt;

&lt;h3 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h3&gt;

&lt;p&gt;Watch the video below for a presentation on this material:&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=9GZRw3MA730&quot;&gt;
&lt;img src=&quot;/res/blog_16/rez-01.png&quot; /&gt;
&lt;/a&gt;﻿&lt;/p&gt;

&lt;h3 id=&quot;notes&quot;&gt;NOTES&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;This was written in Python and D3.js : see code &lt;a href=&quot;https://github.com/NicholasARossi/VizSnacks&quot;&gt;here&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;The data was retrieved from &lt;a href=&quot;https://www.icao.int/safety/iStars/Pages/API-Data-Service.aspx&quot;&gt;here&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;The gooey effect was taken from Nadieh Bremmers work &lt;a href=&quot;http://bl.ocks.org/nbremer/8df57868090f11e59175804e2062b2aa&quot;&gt;here&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</content>
    <author>
      <name>Nicholas Rossi</name>
      <uri>http://nicholasarossi.github,io/about/</uri>
    </author>
  </entry>
  
  <entry>
    <title>Lego Datascience</title>
    <link href="http://nicholasarossi.github.io//Lego"/>
    <updated>2018-05-01T00:00:00-07:00</updated>
    <id>http://nicholasarossi.github.io//Lego</id>
    <content type="html">&lt;style&gt;
@font-face {
  font-family: 'futura';
  src: url('/res/blog_10/futura.ttf') format('truetype');
  font-weight: normal;
  font-style: normal;
}
      h1,h2,h3,head,title {
        font-family: 'futura';
        font-weight: 1000;
        outline-color: black;
        color: black;
        background-color: #FF8888;
        text-shadow:
        -.75px -.75px 0 #000,
        .75px -.75px 0 #000,
        -.75px .75px 0 #000,
        .75px .75px 0 #000;
}
		iframe {
			width: 10px;
			min-width: 100%;
			*width: 100%;
		}
&lt;/style&gt;

&lt;iframe src=&quot;/res/blog_10/lego_dance2.html&quot; height=&quot;200px&quot; width=&quot;100%&quot; scrolling=&quot;no&quot;&gt;&lt;/iframe&gt;

&lt;p&gt;The Danish building toy has come a long way from its humble roots. From the 1950s to today,
LEGO has expanded to include an ever increasing variety of kit sizes. The animation below summarizes how
the company has exploded over 7 decades.&lt;/p&gt;

&lt;video controls=&quot;&quot; loop=&quot;&quot; autoplay=&quot;&quot; width=&quot;100%&quot;&gt;
&lt;source src=&quot;/res/blog_10/submit.webm&quot; autoplay=&quot;true&quot; type=&quot;video/webm&quot; /&gt;
 &lt;source src=&quot;/res/blog_10/submit.mp4&quot; autoplay=&quot;true&quot; type=&quot;video/mp4&quot; /&gt;
&lt;/video&gt;

&lt;p&gt;This video contains tons of information, so let’s take it piece by piece.&lt;/p&gt;

&lt;h3 id=&quot;distributions-over-time&quot;&gt;DISTRIBUTIONS OVER TIME&lt;/h3&gt;

&lt;p&gt;We want to know not only how the mean kit size has changes, but also how the variety of kit sizes has changed.
To do that we need to overlay distributions.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/res/blog_10/two_pannel.png&quot;&gt;
&lt;img src=&quot;/res/blog_10/two_pannel.png&quot; /&gt;
&lt;/a&gt;﻿&lt;/p&gt;

&lt;p&gt;However this is sort of a mess, I think we can clean it up though using cumulative distributions instead of normal distributions.
&lt;a href=&quot;https://en.wikipedia.org/wiki/Cumulative_distribution_function&quot;&gt;Cumulative distributions&lt;/a&gt; are just density functions (or histograms)
that are summed along the x-axis. To play with that idea click on the interactive below that toggles between the normal histogram
and the cumulative histogram for all lego kits ever made.&lt;/p&gt;

&lt;iframe src=&quot;/res/blog_10/line_transition2.html&quot; width=&quot;100%&quot; height=&quot;300px&quot; scrolling=&quot;no&quot;&gt;&lt;/iframe&gt;

&lt;p&gt;We see that they encode the same information! In the animation above demonstrates how it’s easier to see the differences in the cumulative distribution over time.&lt;/p&gt;

&lt;h3 id=&quot;from-starwars-to-bionicle--lego-themes&quot;&gt;FROM STARWARS TO BIONICLE : LEGO THEMES&lt;/h3&gt;

&lt;p&gt;So as the kits themselves explode over time so do the variety of themes. There are an incredible variety of lego themes
even though the majority have less than ten kits in each. See the figure for how the number of themes has expanded as well as the
top themes (by number of sets) of all time.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/res/blog_10/themes.png&quot;&gt;
&lt;img src=&quot;/res/blog_10/themes.png&quot; /&gt;
&lt;/a&gt;﻿&lt;/p&gt;

&lt;h3 id=&quot;conclusion&quot;&gt;CONCLUSION&lt;/h3&gt;

&lt;p&gt;See the video below for more information on this:&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=COhT9Bj3yLc&quot;&gt;
&lt;img src=&quot;/res/blog_10/maxresdefault.jpg&quot; /&gt;
&lt;/a&gt;﻿&lt;/p&gt;

&lt;h3 id=&quot;notes&quot;&gt;NOTES&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;This was written in Python and D3.js : see code &lt;a href=&quot;https://github.com/NicholasARossi/lego&quot;&gt;here&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;The data was retrieved from &lt;a href=&quot;https://www.kaggle.com/rtatman/lego-database&quot;&gt;here&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</content>
    <author>
      <name>Nicholas Rossi</name>
      <uri>http://nicholasarossi.github,io/about/</uri>
    </author>
  </entry>
  
  <entry>
    <title>The Colors of National Geographic</title>
    <link href="http://nicholasarossi.github.io//NatGeo"/>
    <updated>2018-04-20T00:00:00-07:00</updated>
    <id>http://nicholasarossi.github.io//NatGeo</id>
    <content type="html">&lt;style&gt;
@font-face {
  font-family: 'permanent markercmd';
  src: url('/res/blog_12/PermanentMarker.ttf') format('truetype');
  font-weight: normal;
  font-style: normal;
}
  h1,h2,h3,head,title {
    font-family: 'permanent marker',sans-serif;
    color: Black;
  }
&lt;/style&gt;

&lt;h1 id=&quot;what-are-the-common-colors-of-nature&quot;&gt;What are the common colors of nature?&lt;/h1&gt;

&lt;iframe src=&quot;/res/blog_14/pixel-smear/smear.html&quot; width=&quot;100%&quot; height=&quot;250px&quot; scrolling=&quot;no&quot;&gt;&lt;/iframe&gt;

&lt;p&gt;In this project, I wanted to dig into photos of the earth to see what colors were the most commom.
National Geographic seemed like a good group to cross-section the natural world. I think if there is one group that can tell us what earth looks like, it’s them.&lt;/p&gt;

&lt;h1 id=&quot;mining-the-data&quot;&gt;Mining the data&lt;/h1&gt;
&lt;p&gt;After scraping all the photos from national geographic’s instagram account, I used &lt;a href=&quot;https://en.wikipedia.org/wiki/K-means_clustering&quot;&gt;k-means clustering&lt;/a&gt; to find the primary 5 colors of each photo. K-means is a mechanism of finding groups in continuous space.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/res/blog_14/girl.png&quot;&gt;
&lt;img src=&quot;/res/blog_14/girl.png&quot; /&gt;
&lt;/a&gt;﻿&lt;/p&gt;

&lt;p&gt;I then repeated this process across all photos that I pulled from instagram.&lt;/p&gt;

&lt;h1 id=&quot;cleaning-and-plotting-the-data&quot;&gt;Cleaning and Plotting the Data&lt;/h1&gt;
&lt;p&gt;Ok so now I have the primary colors over all the photos now what? The goal is to plot these is a way that’s representatitive of the entire set.&lt;/p&gt;

&lt;p&gt;Color is a multi-dimensional thing. Typically we think of things in terms of Red, Blue and Green values (or CMYK). Alternatively you can consider color in terms of Hue, Saturation and Value.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/res/blog_14/hsv-01.png&quot; alt=&quot;HSV&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Considering the image above from &lt;a href=&quot;https://en.wikipedia.org/wiki/HSL_and_HSV&quot;&gt;wikipedia&lt;/a&gt; we see that Hue is the apparent absolute rainbow color, saturation is the &lt;em&gt;whiteness&lt;/em&gt; and value is the &lt;em&gt;blacknesss&lt;/em&gt;. With these three values you can express any color.&lt;/p&gt;

&lt;p&gt;I wasn’t interested in black and white photos, I wanted to look at color so I decided I wanted to plot hue. So using the data scraped, I discared the greyscale colors and generated a histogram of all the hues.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;hlist&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;group&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;storage&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;#Loop over all photos
&lt;/span&gt;    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;color&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;group&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;#Loop over all colors for each photo
&lt;/span&gt;        &lt;span class=&quot;n&quot;&gt;h&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;v&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_hsv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;color&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.1&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;and&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;v&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;#disgard low saturation and value colors
&lt;/span&gt;            &lt;span class=&quot;n&quot;&gt;hlist&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;h&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;bins&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;arange&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.01&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1.01&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.01&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;probs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bons&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;histogram&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hlist&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;normed&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bins&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bins&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;To make it &lt;em&gt;extra&lt;/em&gt; pretty I just wrapped the histogram circularly and interpolated along the curve to make it nice and smooth.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/res/blog_14/radial_hist.png&quot;&gt;
&lt;img src=&quot;/res/blog_14/radial_hist.png&quot; /&gt;
&lt;/a&gt;﻿&lt;/p&gt;

&lt;p&gt;Et voila! Those are the most predominant hues in Natgeo’s instagram photos.&lt;/p&gt;

&lt;h3 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h3&gt;

&lt;p&gt;Natgeo takes a lot of photos of people, and the peak of &lt;em&gt;orangish&lt;/em&gt; colors could be picking that. Next step is to remove the people photos and then sort by geo tags to see what the colors of particular places are.&lt;/p&gt;

&lt;h3 id=&quot;notes&quot;&gt;Notes:&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;THis was an extension of the blogpost I read &lt;a href=&quot;http://charlesleifer.com/blog/using-python-and-k-means-to-find-the-dominant-colors-in-images/&quot;&gt;here&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Code for batch color analysis and plotting is available &lt;a href=&quot;https://github.com/NicholasARossi/color_me_impressed&quot;&gt;here&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;The data was taken from Natgeo’s instagram account using &lt;a href=&quot;https://github.com/althonos/InstaLooter&quot;&gt;instalooter&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</content>
    <author>
      <name>Nicholas Rossi</name>
      <uri>http://nicholasarossi.github,io/about/</uri>
    </author>
  </entry>
  
  <entry>
    <title>How to Lie with Histograms</title>
    <link href="http://nicholasarossi.github.io//LinLog"/>
    <updated>2018-03-15T00:00:00-07:00</updated>
    <id>http://nicholasarossi.github.io//LinLog</id>
    <content type="html">&lt;html&gt;
&lt;style&gt;
@font-face {
  font-family: 'SFCartoonistHand';
  src: url('/res/blog_13/SFCartoonistHand.ttf') format('truetype');
  font-weight: normal;
  font-style: normal;
}
  h1,h2,h3,h4,h5,h6,head,title,pre, code, tt {
    font-family: 'SFCartoonistHand',serif;
    color: white;
    background-color: turquoise;
  }
&lt;/style&gt;
&lt;/html&gt;

&lt;h3 id=&quot;same-data-different-stories&quot;&gt;Same Data, Different Stories&lt;/h3&gt;
&lt;p&gt;One of the most foundational techniques used in visualization is the histogram. This type of figure allows us to look at populations of numerical data and intuit how values are distributed,
as well as what the general statistics of the system are. Basically it’s a way of looking how common something is. The only problem with them is they’re easily manipulated. You can bamboozle someone easily depending on how you plot them.
This comes from the fact that data in histograms is &lt;strong&gt;binned&lt;/strong&gt; - that mean you put the data in little categories in order to plot it.&lt;/p&gt;

&lt;p&gt;As a case study lets check out some data on undergraduate acceptance rates.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/res/blog_13/acceptance_hist.png&quot;&gt;
&lt;img src=&quot;/res/blog_13/acceptance_hist.png&quot; /&gt;
&lt;/a&gt;﻿&lt;/p&gt;

&lt;p&gt;Now in this case, we have linearly spaced bins between 0 and 100% acceptance rates. It demonstrates clearly how the majority of schools accept the majority of students and how elite, picky institutions are relatively rare.&lt;/p&gt;

&lt;p&gt;The first failure point in the presentation of this data is the scale. Projecting the same data onto a logarithmic x-axis produces a qualitatively different impression from the data.&lt;/p&gt;

&lt;video controls=&quot;&quot; loop=&quot;&quot; autoplay=&quot;&quot; width=&quot;100%&quot;&gt;
&lt;!-- &lt;source src=&quot;/res/blog_13/seed_logo.webm&quot; autoplay=&quot;true&quot; type=&quot;video/webm&quot;&gt; --&gt;
&lt;source src=&quot;/res/blog_13/schoollinlog.mp4&quot; autoplay=&quot;true&quot; type=&quot;video/mp4&quot; /&gt;

Your browser does not support the video tag.
&lt;/video&gt;

&lt;p&gt;Even without considering histograms, granular data itself changes its qualitative impression through the lense of a different axis.
Consider the interactive figure below to see how a simple, normally distributed random variable looks different on these two scales.&lt;/p&gt;

&lt;iframe src=&quot;/res/blog_13/linear_log/index.html&quot; width=&quot;100%&quot; height=&quot;200px&quot; scrolling=&quot;no&quot;&gt;&lt;/iframe&gt;

&lt;p&gt;So why would we ever want to use anything besides linear scales if they change the qualitative impression of the data? Well, certain data sets lose a lot of their nuance when  projected onto
linear axis. Consider the distribution of U.S. firm sizes. There are lots of big firms and a few small ones. This doesn’t look great on a linear axis.&lt;/p&gt;

&lt;video controls=&quot;&quot; loop=&quot;&quot; autoplay=&quot;&quot; width=&quot;100%&quot;&gt;
&lt;!-- &lt;source src=&quot;/res/blog_13/seed_logo.webm&quot; autoplay=&quot;true&quot; type=&quot;video/webm&quot;&gt; --&gt;
&lt;source src=&quot;/res/blog_13/firms_linlog.mp4&quot; autoplay=&quot;true&quot; type=&quot;video/mp4&quot; /&gt;

Your browser does not support the video tag.
&lt;/video&gt;

&lt;h3 id=&quot;scaling-the-bins&quot;&gt;Scaling the bins&lt;/h3&gt;

&lt;p&gt;Walking back a bit - it’s important to note that the bins selected for histograms are entirely arbitrary. It’s up to the visualization author to
decide which ones to choose. In fact logarithmically scaled bins (bins that appear uniform on a logarithmic scale) will present very different impressions.&lt;/p&gt;

&lt;p&gt;The simplest example of this would be a uniform distribution. Presented two bin-scales gives two very distinct impressions.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/res/blog_13/uniform_hist.png&quot;&gt;
&lt;img src=&quot;/res/blog_13/uniform_hist.png&quot; /&gt;
&lt;/a&gt;﻿&lt;/p&gt;

&lt;p&gt;This extends to other types of statistical distributions like the gamma distribution…&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/res/blog_13/test_hist.png&quot;&gt;
&lt;img src=&quot;/res/blog_13/test_hist.png&quot; /&gt;
&lt;/a&gt;﻿&lt;/p&gt;

&lt;p&gt;As well as real data. Below we see the distributions of the &lt;strong&gt;sizes of global cities and towns&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/res/blog_13/data_hist.png&quot;&gt;
&lt;img src=&quot;/res/blog_13/data_hist.png&quot; /&gt;
&lt;/a&gt;﻿&lt;/p&gt;

&lt;h3 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h3&gt;

&lt;p&gt;So clearly there is a lot of subjectivity in how distributions are presented with histograms. This comes in many ways because there is no truely &lt;em&gt;right&lt;/em&gt; way to do it.
There are only a few guidelines:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;If you’re &lt;strong&gt;consuming&lt;/strong&gt; data, pay attention to the pitfalls that can happen here. If the author is unclear about how they binned or projected the data it’s an immediate red flag.&lt;/li&gt;
  &lt;li&gt;If you’re &lt;strong&gt;creating&lt;/strong&gt; data, help out the viewer by drawing attention to your axis if you do anything besides a linear scale.&lt;/li&gt;
  &lt;li&gt;If you have the ability, try to find and present the granular data (individual points) it’s always better.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Watch the video essay below for further analysis:
&lt;a href=&quot;https://www.youtube.com/watch?v=m2lPYvrPlrY&quot;&gt;
&lt;img src=&quot;/res/blog_13/hist_thumb.jpg&quot; /&gt;
&lt;/a&gt;﻿&lt;/p&gt;

&lt;h3 id=&quot;notes&quot;&gt;Notes:&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;This was written in Python and D3.js : see code &lt;a href=&quot;https://github.com/NicholasARossi/log_lin_distribution_comparisons&quot;&gt;here&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;The city data retrieved from &lt;a href=&quot;https://simplemaps.com/data/world-cities&quot;&gt;here&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

</content>
    <author>
      <name>Nicholas Rossi</name>
      <uri>http://nicholasarossi.github,io/about/</uri>
    </author>
  </entry>
  
  <entry>
    <title>Shared Actors between the 2018 Best Picture Nominees</title>
    <link href="http://nicholasarossi.github.io//Academy"/>
    <updated>2018-03-01T00:00:00-08:00</updated>
    <id>http://nicholasarossi.github.io//Academy</id>
    <content type="html">&lt;script src=&quot;https://cdn.jsdelivr.net/npm/lazyload@2.0.0-beta.2/lazyload.js&quot;&gt;&lt;/script&gt;

&lt;html&gt;
&lt;link rel=&quot;stylesheet&quot; href=&quot;https://fonts.googleapis.com/css?family=SFCar&quot; /&gt;
&lt;style&gt;
@font-face {
  font-family: 'ebgaramond';
  src: url('/res/blog_12/EBGaramond.ttf') format('truetype');
  font-weight: normal;
  font-style: normal;
}
  h1,h2,h3,head,title {
    font-family: 'ebgaramond',serif;
    color: Black;
    background-color: yellow;
  }
&lt;/style&gt;
&lt;/html&gt;

&lt;h3 id=&quot;familiar-faces&quot;&gt;Familiar Faces&lt;/h3&gt;

&lt;p&gt;There are a lot of recurrent actors in this year’s best picture nominees. The  network diagram below shows how they connect.
&lt;a href=&quot;/res/blog_12/cluster.png&quot;&gt;
&lt;img class=&quot;lazyload&quot; src=&quot;/res/blog_12/cluster.png&quot; /&gt;
&lt;/a&gt;﻿&lt;/p&gt;

&lt;p&gt;The most outrageous example of this is &lt;a href=&quot;https://en.wikipedia.org/wiki/Michael_Stuhlbarg&quot;&gt;Michael Stuhlbarg&lt;/a&gt;. He appears in “The Post”, “The Shape of Water” and “Call Me by Your Name”. If the academy awards were purely random, he would have a 33% chance of being in a movie that would win best picture!&lt;/p&gt;

&lt;p&gt;Breaking this down a bit, let’s just look at this year’s two WWII epics : Dunkirk and Darkest Hour. There’s only one actor that unites the two films. Mouse over for the complete cast list.&lt;/p&gt;

&lt;iframe class=&quot;lazyload&quot; src=&quot;/res/blog_12/cluster_ww2.html&quot; width=&quot;100%&quot; height=&quot;300px&quot; scrolling=&quot;no&quot;&gt;&lt;/iframe&gt;

&lt;p&gt;Phantom thread is on it’s own, having shared no actors with any other films. The main cluster contains the remain 6 films and can be investigated below:&lt;/p&gt;

&lt;iframe class=&quot;lazyload&quot; src=&quot;/res/blog_12/other_sets.html&quot; width=&quot;100%&quot; height=&quot;500px&quot; scrolling=&quot;no&quot;&gt;&lt;/iframe&gt;

&lt;h3 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h3&gt;
&lt;p&gt;Just as with action stars getting reused in blockbuster cinema, it appears there is a culture of employing the same actors in the award targeting niche.&lt;/p&gt;

&lt;h3 id=&quot;notes&quot;&gt;Notes:&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;This was written in Python and D3.js : see code &lt;a href=&quot;https://github.com/NicholasARossi/academy_awards/tree/master&quot;&gt;here&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;The data was taken from &lt;a href=&quot;https://www.themoviedb.org&quot;&gt;the movie db&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</content>
    <author>
      <name>Nicholas Rossi</name>
      <uri>http://nicholasarossi.github,io/about/</uri>
    </author>
  </entry>
  
  <entry>
    <title>Schools are still segregated</title>
    <link href="http://nicholasarossi.github.io//segregation"/>
    <updated>2018-01-01T00:00:00-08:00</updated>
    <id>http://nicholasarossi.github.io//segregation</id>
    <content type="html">&lt;script src=&quot;https://cdn.jsdelivr.net/npm/lazyload@2.0.0-beta.2/lazyload.js&quot;&gt;&lt;/script&gt;

&lt;meta property=&quot;og:image&quot; content=&quot;/res/blog_8/thumb2.png&quot; /&gt;

&lt;html&gt;
  &lt;head&gt;
    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://fonts.googleapis.com/css?family=Permanent+Marker&quot; /&gt;
    &lt;style&gt;
      &lt;!-- h1,h2,h3,title {
        font-family: 'Permanent Marker',serif;
        color: darkturquoise;
      } --&gt;
      h1,h2,h3,h4,h5,h6,pre, code, tt {
        font-family: 'Permanent Marker',serif;
        color: darkturquoise;
      }
      h1,head,title{
        background-color: magenta;
        color:white;
      }

      body {
        &lt;!-- font-family: 'timesnewroman',serif; --&gt;
        color: black;
      }
    &lt;/style&gt;
  &lt;/head&gt;
&lt;/html&gt;
&lt;script type=&quot;text/x-mathjax-config&quot;&gt;
  MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
&lt;/script&gt;

&lt;script type=&quot;text/javascript&quot; async=&quot;&quot; src=&quot;https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_CHTML&quot;&gt;
&lt;/script&gt;

&lt;h2 id=&quot;segregation-is-dead-long-live-segregation&quot;&gt;Segregation is dead, long live segregation&lt;/h2&gt;

&lt;p&gt;When I think of segregation, I think of a historical event. Something begun under the umbrella of government agency and vanquished by collective endeavor. However, the data suggest that despite the end of exogenous state-sponsored inequity, we see a emergent clustering of ethnic groups in America that produces non-homogenous demographic distributions in k-12 education. Below we investigate the scope of this phenomenon using a number of quantitative techniques.&lt;/p&gt;

&lt;h2 id=&quot;demographics-are-distributed-regionally-then-locally&quot;&gt;Demographics are distributed regionally, then locally&lt;/h2&gt;
&lt;p&gt;Before we evaluate local clustering in school populations, we must begin by decoupling it regional phenomena. Not every state has the same demographics. There exists majority minority states (Hawaii, California, Texas) and majority white states (Vermont, Maine, West Virginia). We are a collective of vaguely autonomous republics and any evaluation of a student body of a particular school must be contextualized by the statistics of the state in which it is located. Below we see the white proportionality of each state:&lt;/p&gt;

&lt;iframe class=&quot;lazyload&quot; src=&quot;/res/blog_8/map.html&quot; width=&quot;100%&quot; height=&quot;50%&quot; scrolling=&quot;no&quot;&gt;&lt;/iframe&gt;

&lt;h2 id=&quot;what-does-modern-segregation-look-like&quot;&gt;What does modern segregation look like?&lt;/h2&gt;

&lt;p&gt;Unlike de jure segregation of the past, modern racial clustering is not &lt;em&gt;usually&lt;/em&gt; a consequence of direct state action, but rather an emergent phenomenon arising from the interface of human behavior and public policy. In fact &lt;a href=&quot;https://en.wikipedia.org/wiki/Parents_Involved_in_Community_Schools_v._Seattle_School_District_No._1&quot;&gt;the supreme court ruled in 2007&lt;/a&gt; that it was unconstitutional for state actors to forcibly integrate schools - so racial separation today is a passive, bottom-up process.&lt;/p&gt;

&lt;p&gt;To examine the face of modern segregation let’s consider the following two high-schools : &lt;span style=&quot;color:darkturquoise&quot;&gt;&lt;strong&gt;Minnetonka High&lt;/strong&gt;&lt;/span&gt; just outside of Minneapolis, MN and &lt;span style=&quot;color:magenta&quot;&gt;&lt;strong&gt;Berkmar High&lt;/strong&gt;&lt;/span&gt; just outside of Atlanta, GA.
Minnetonka is predominantly white and Berkmar is predominantly non-white, but that alone is not enough to indicate &lt;em&gt;segragation&lt;/em&gt;.&lt;/p&gt;

&lt;iframe class=&quot;lazyload&quot; src=&quot;/res/blog_8/schools.html&quot; width=&quot;100%&quot; height=&quot;300px&quot; scrolling=&quot;yes&quot;&gt;&lt;/iframe&gt;

&lt;p&gt;The question is - how well do the demographics of these schools mirror the demographics of the state? Are they representative of a well mixed population?
To gain context, lets consider a &lt;span style=&quot;color:darkgrey&quot;&gt;&lt;strong&gt;hypothetical distribution&lt;/strong&gt;&lt;/span&gt; of schools that comes from random resampling of the total state demographics. Here, we would expect some variance in the demographics of the schools as a function of the the error in bootstrap resampling. However, we can see that the distribution remains relatively homogenous with schools neither being purely white nor purely non-white.  Reality is very far from this randomly generated distribution. For both &lt;span style=&quot;color:darkturquoise&quot;&gt;&lt;strong&gt;Minnesota&lt;/strong&gt;&lt;/span&gt; and &lt;span style=&quot;color:magenta&quot;&gt;&lt;strong&gt;Georgia&lt;/strong&gt;&lt;/span&gt;, we see many schools are not well mixed - the ensemble of schools expresses a heterogeneous range of demographics.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/res/blog_8/MN_GA_density.png&quot;&gt;
&lt;img class=&quot;lazyload&quot; src=&quot;/res/blog_8/MN_GA_density.png&quot; /&gt;
&lt;/a&gt;﻿&lt;/p&gt;

&lt;p&gt;The qualitative difference are clear, but what quantitative metrics can we employ to compare the relative &lt;em&gt;segregation&lt;/em&gt; of each state.&lt;/p&gt;

&lt;p&gt;First lets borrow a tool from information theory &lt;a href=&quot;https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence&quot;&gt;Kullback Leibler divergence&lt;/a&gt;. This is a non-parametric calculation of how similar two distribution are. In this case, we can compare how similar the actual demographic distributions of schools in a state are to the distribution that would be generated from pure random sampling.&lt;/p&gt;

&lt;video controls=&quot;&quot; loop=&quot;&quot; autoplay=&quot;&quot; width=&quot;100%&quot; height=&quot;300&quot;&gt;
&lt;source src=&quot;/res/blog_8/mass_data.webm&quot; autoplay=&quot;true&quot; type=&quot;video/webm&quot; /&gt;
&lt;source src=&quot;/res/blog_8/mass_data.mp4&quot; autoplay=&quot;true&quot; type=&quot;video/mp4&quot; /&gt;

&lt;center&gt;
\[
D_{kL}(P||Q)= \int^{\infty}_{-\infty} p(x)log\frac{p(x)}{q(x)} \, dx
\]
&lt;/center&gt;
&lt;/video&gt;

&lt;p&gt;Alternatively, we can solve the binomial proportion confidence interval formula for how unlikely randomly sampling a school population would be. In this case $\hat{p}(x)$ is the observed percentage of white people in a school, ${p}(x)$ is the state wide percentage, and n is the number of students in that school. The z score can be understood as a measure of unlikeliness that that school population could occur randomly - or how segregated it is.&lt;/p&gt;
&lt;center&gt;
\[
z=\frac{|\hat{p}(x)-p(x)|}{\sqrt{\frac{1}{n}(p(x)*(1-p(x)))}}
\]
&lt;/center&gt;
&lt;p&gt;Then we can average that over all k schools in the state.&lt;/p&gt;
&lt;center&gt;
\[
\bar{z}=\frac{1}{k}\sum_{j=1}^{k}\frac{|\hat{p}_j(x)-p_j(x)|}{\sqrt{\frac{1}{n}(p_j(x)*(1-p_j(x)))}}
\]
&lt;/center&gt;

&lt;p&gt;&lt;a href=&quot;/res/blog_8/MN_GA_quant.png&quot;&gt;
&lt;img class=&quot;lazyload&quot; src=&quot;/res/blog_8/MN_GA_quant.png&quot; /&gt;
&lt;/a&gt;﻿&lt;/p&gt;

&lt;p&gt;We see that both metric confirm similar trends : that Minnesota is less segregated for its demographics than Georgia.&lt;/p&gt;

&lt;h3 id=&quot;segregation-manifests-spatially&quot;&gt;Segregation manifests spatially&lt;/h3&gt;
&lt;p&gt;An important element of these data is the spatial component. See the maps below of the first and fourth most populous states and how their school demographics are distributed spatially.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/res/blog_8/california.png&quot;&gt;
&lt;img class=&quot;lazyload&quot; src=&quot;/res/blog_8/california.png&quot; /&gt;
&lt;/a&gt;﻿&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/res/blog_8/ny.png&quot;&gt;
&lt;img class=&quot;lazyload&quot; src=&quot;/res/blog_8/ny.png&quot; /&gt;
&lt;/a&gt;﻿&lt;/p&gt;

&lt;p&gt;New York shows that certain upstate is disproportionately white while the city is disproportionately nonwhite. Interestingly, even within the city there exists demographic heterogeneity manifested as local clustering.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/res/blog_8/manhattan.png&quot;&gt;
&lt;img class=&quot;lazyload&quot; src=&quot;/res/blog_8/manhattan.png&quot; /&gt;
&lt;/a&gt;﻿&lt;/p&gt;

&lt;h2 id=&quot;verdict--how-segregated-is-your-state&quot;&gt;Verdict : how segregated is your state?&lt;/h2&gt;

&lt;p&gt;Applying the z-score metric from above we can calculate the average degree of segregation for each state.&lt;/p&gt;

&lt;iframe src=&quot;/res/blog_8/map3.html&quot; width=&quot;100%&quot; height=&quot;50%&quot; scrolling=&quot;no&quot;&gt;&lt;/iframe&gt;
&lt;!-- &lt;iframe src=&quot;/res/blog_8/states_scatter.html&quot; width=&quot;100%&quot;  height=&quot;100%&quot;  scrolling=&quot;no&quot;&gt;&lt;/iframe&gt; --&gt;

&lt;h2 id=&quot;what-predicts-this-segregation&quot;&gt;What predicts this segregation?&lt;/h2&gt;
&lt;p&gt;We can consider several covariates with this emergent, de-facto, segregation. First, the demographics of the state matter, as you can’t be segregated if you’re completely homogenous. Mouse over the state below to reveal their &lt;span style=&quot;color:teal&quot;&gt;&lt;strong&gt;% white&lt;/strong&gt;&lt;/span&gt; as well as their &lt;span style=&quot;color:magenta&quot;&gt;&lt;strong&gt;segregation score&lt;/strong&gt;&lt;/span&gt;.&lt;/p&gt;

&lt;style&gt;.embed-container { position: relative; padding-bottom: 85%; height: 0; overflow: hidden; max-width: 100%; } .embed-container iframe, .embed-container object, .embed-container embed { position: absolute; top: 0; left: 0; width: 100%; height: 100%; }&lt;/style&gt;
&lt;div class=&quot;embed-container&quot;&gt;&lt;iframe src=&quot;/res/blog_8/states_scatter.html&quot; frameborder=&quot;0&quot; allowfullscreen=&quot;&quot; scrolling=&quot;no&quot;&gt;&lt;/iframe&gt;&lt;/div&gt;

&lt;p&gt;However, this variable doesn’t have strong predictive power over whether the state is segregated or not. The level of urbanization of the state seems to have more of an impact. Mouse over the state below to reveal their &lt;span style=&quot;color:coral&quot;&gt;&lt;strong&gt;% urbanized&lt;/strong&gt;&lt;/span&gt; as well as their &lt;span style=&quot;color:slategrey&quot;&gt;&lt;strong&gt;segregation score&lt;/strong&gt;&lt;/span&gt;.&lt;/p&gt;

&lt;style&gt;.embed-container { position: relative; padding-bottom: 85%; height: 0; overflow: hidden; max-width: 100%; } .embed-container iframe, .embed-container object, .embed-container embed { position: absolute; top: 0; left: 0; width: 100%; height: 100%; }&lt;/style&gt;
&lt;div class=&quot;embed-container&quot;&gt;&lt;iframe src=&quot;/res/blog_8/states_scatter2.html&quot; frameborder=&quot;0&quot; allowfullscreen=&quot;&quot; scrolling=&quot;no&quot;&gt;&lt;/iframe&gt;&lt;/div&gt;

&lt;h3 id=&quot;afterword&quot;&gt;Afterword&lt;/h3&gt;
&lt;p&gt;US government data is highly racialized which lends itself to testing these sorts of hypotheses. For instance, tracking distributions of student family income is a little harder. However, statistics like income could be inferred from proxy variables like the percentage of students qualifying in free lunch programs.&lt;/p&gt;

&lt;p&gt;Still, these data represent quantitatively disprove the hypothesis that segregation is exclusively a top-down phenomenon that was part of the historical record. It still exists today in part because of the emergent clustering produced from humans tendency to self-associate.&lt;/p&gt;

&lt;p&gt;Watch the video essay below for further analysis:
&lt;a href=&quot;https://www.youtube.com/watch?v=VUwzI4DOYOU&quot;&gt;
&lt;img class=&quot;lazyload&quot; src=&quot;/res/blog_8/thumb.png&quot; /&gt;
&lt;/a&gt;﻿&lt;/p&gt;

&lt;!-- &lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/watch?v=uszdal33edo&quot;&gt;&lt;/iframe&gt; --&gt;

&lt;h3 id=&quot;notes&quot;&gt;Notes:&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Data can be found &lt;a href=&quot;https://ocrdata.ed.gov/&quot;&gt;here&lt;/a&gt;; the urbanization and state demographics just from wikipedia&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/NicholasARossi/Segregation&quot;&gt;Code&lt;/a&gt; ; maps were made with &lt;a href=&quot;https://matplotlib.org/basemap/&quot;&gt;basemap&lt;/a&gt; - ideas for the map figures came from &lt;a href=&quot;http://ramiro.org/notebook/new-york-roads-railways/&quot;&gt;here&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</content>
    <author>
      <name>Nicholas Rossi</name>
      <uri>http://nicholasarossi.github,io/about/</uri>
    </author>
  </entry>
  
</feed>
